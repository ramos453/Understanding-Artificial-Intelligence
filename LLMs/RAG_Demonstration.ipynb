{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a37b15",
   "metadata": {},
   "source": [
    "__RAG__\n",
    "\n",
    "This notebook shows a simple example of Retrieval-Augmented Generation (RAG). The goal is to answer questions with specific user information. In this way, a traditional LLM is enhanced by having access to proprietary data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f702099",
   "metadata": {},
   "source": [
    "Retrieve \n",
    "- When a user asks a question, a search for additional information to a private database is made \n",
    "\n",
    "Augment\n",
    "- The information gathered is then used to \"augment\" the original questions by providing more context\n",
    "\n",
    "\n",
    "Generate \n",
    "- The context + question is sent to an LLM, which in turn can generate a more accurate answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8289baa",
   "metadata": {},
   "source": [
    "The next cell imports packages needed for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3481b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ace4e",
   "metadata": {},
   "source": [
    "Private data can be stored in different ways. For this simple example we will store our private database as text strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "24704dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base created with 5 documents.\n"
     ]
    }
   ],
   "source": [
    "knowledge_base = [\n",
    "    \"Pablo once hiked Angel's Landing in Zion National Park. It is one of the deadliest hikes in the US.\",\n",
    "    \"Pablo studied abroad in Rome, Italy where he took classical art history courses.\",\n",
    "    \"Pablo enjoys playing tennis, pickleball, soccer and hiking. He plays tennis at the local courts every Thursday.\",\n",
    "    \"Pablo has a pet dog, a beagle that he adopted from a shelter in 2018.\",\n",
    "    \"Pablo enjoys reggaeton music and has seen Pitbull in concert twice.\",\n",
    "]\n",
    "\n",
    "print(f\"Knowledge base created with {len(knowledge_base)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704a8dd0",
   "metadata": {},
   "source": [
    "In order to get the word meanings, we need to represent the words as embeddings as seen in the previous project. For this example, we will use a pertained model to create these embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9fbdffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings created for the knowledge base.\n",
      "Shape of the embeddings tensor: torch.Size([5, 384])\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained model for creating embeddings\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Create embeddings for our knowledge base\n",
    "knowledge_base_embeddings = embedding_model.encode(knowledge_base, convert_to_tensor=True)\n",
    "\n",
    "print(\"Embeddings created for the knowledge base.\")\n",
    "print(\"Shape of the embeddings tensor:\", knowledge_base_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927f898a",
   "metadata": {},
   "source": [
    "Below are the embeddings created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d7d8cf17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0815,  0.0336, -0.0262,  ..., -0.0824, -0.0435, -0.0410],\n",
       "        [ 0.0509, -0.0035,  0.0167,  ...,  0.0033, -0.0289, -0.0178],\n",
       "        [ 0.1076,  0.0102,  0.0461,  ..., -0.0350, -0.0333,  0.0059],\n",
       "        [-0.0010, -0.0361,  0.0553,  ...,  0.0012,  0.0890,  0.0519],\n",
       "        [ 0.0871, -0.0851,  0.0171,  ..., -0.0459,  0.1151, -0.0166]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knowledge_base_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f47c00c",
   "metadata": {},
   "source": [
    "The cell below poses a question. \"What type of activities does Pablo enjoy?\". \n",
    "\n",
    "Then we compare the questions embedding to that of the documents in our private database. The goal is to return the most relevant document based on the question. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cfa52bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Question: What type of activities does Pablo enjoy?\n",
      "Most relevant document found (Score: 0.7002):\n",
      "---\n",
      "Pablo enjoys playing tennis, pickleball, soccer and hiking. He plays tennis at the local courts every Thursday.\n"
     ]
    }
   ],
   "source": [
    "# what would be a better example question to ask for this knowledge base?\n",
    "user_question = \"What type of activities does Pablo enjoy?\"\n",
    "\n",
    "# 1. Create an embedding for the user's question\n",
    "question_embedding = embedding_model.encode(user_question, convert_to_tensor=True)\n",
    "\n",
    "# 2. Calculate cosine similarity between the question and all knowledge base documents\n",
    "cos_scores = util.cos_sim(question_embedding, knowledge_base_embeddings)[0]\n",
    "\n",
    "# 3. Find the document with the highest score\n",
    "top_result = torch.argmax(cos_scores)\n",
    "retrieved_context = knowledge_base[top_result]\n",
    "\n",
    "print(f\"User Question: {user_question}\")\n",
    "print(f\"Most relevant document found (Score: {cos_scores[top_result]:.4f}):\")\n",
    "print(\"---\")\n",
    "print(retrieved_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4cf4be",
   "metadata": {},
   "source": [
    "This simple example shows how documents are selected based on questions in RAG.\n",
    "\n",
    "Next we will ask the LLM the same question without providing any additional information from our private database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8f411bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PLAIN PROMPT (NO RAG) ---\n",
      "Question: What type of activities does Pablo enjoy?\n",
      "\n",
      "Answer:\n",
      "\n",
      "--- MODEL'S ANSWER (NO RAG) ---\n",
      "scuba diving\n",
      "\n",
      "--- MODEL'S ANSWER (NO RAG) ---\n",
      "scuba diving\n"
     ]
    }
   ],
   "source": [
    "# The original question without any context\n",
    "plain_prompt = f\"Question: {user_question}\\n\\nAnswer:\"\n",
    "\n",
    "print(\"--- PLAIN PROMPT (NO RAG) ---\")\n",
    "print(plain_prompt)\n",
    "\n",
    "# Generate the answer\n",
    "result_no_rag = generator(plain_prompt, max_new_tokens=50, num_return_sequences=1, do_sample=False)\n",
    "\n",
    "print(\"\\n--- MODEL'S ANSWER (NO RAG) ---\")\n",
    "print(result_no_rag[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae559fb3",
   "metadata": {},
   "source": [
    "Since the model does not inherently have any information on what Pablo's favorite activities are, the model responds with hallucinations. In this case the model guessed that I enjoy scuba diving which is false. \n",
    "\n",
    "\n",
    "Next we will ask the LLM the same prompt and we will provide it with the most relevant document from our private database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bc286888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- AUGMENTED PROMPT ---\n",
      "Answer the question based on the context provided.\n",
      "\n",
      "Context: Pablo enjoys playing tennis, pickleball, soccer and hiking. He plays tennis at the local courts every Thursday.\n",
      "\n",
      "Question: What type of activities does Pablo enjoy?\n",
      "\n",
      "Answer:\n",
      "\n",
      "--- MODEL'S ANSWER (WITH RAG) ---\n",
      "playing tennis, pickleball, soccer and hiking\n",
      "\n",
      "--- MODEL'S ANSWER (WITH RAG) ---\n",
      "playing tennis, pickleball, soccer and hiking\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text2text-generation', model='google/flan-t5-small', torch_dtype=torch.bfloat16)\n",
    "\n",
    "# Augment the prompt with the retrieved context - simplified format for T5\n",
    "augmented_prompt = f\"\"\"Answer the question based on the context provided.\n",
    "\n",
    "Context: {retrieved_context}\n",
    "\n",
    "Question: {user_question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(\"--- AUGMENTED PROMPT ---\")\n",
    "print(augmented_prompt)\n",
    "\n",
    "# Generate the answer\n",
    "# We set max_new_tokens to control only the generated portion\n",
    "result = generator(augmented_prompt, max_new_tokens=50, num_return_sequences=1, do_sample=False)\n",
    "\n",
    "print(\"\\n--- MODEL'S ANSWER (WITH RAG) ---\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665f06de",
   "metadata": {},
   "source": [
    "As shown above, the model with RAG responds correctly, suggesting that I like playing tennis, pickleball, soccer and hiking. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e14d38e",
   "metadata": {},
   "source": [
    "## Additional Considerations\n",
    "\n",
    "This simple example demonstrates the core concept of RAG. In applications such as ChatGPT, RAG may include millions of files from websites, databases, pdf's and more.\n",
    "\n",
    "In our example we used a simple similarity function to find the most relevant document. However, there are many other approaches such as. \n",
    "  - Hybrid search (keyword + semantic)\n",
    "  - Multiple retrieval steps\n",
    "  - Re-ranking retrieved documents\n",
    "  - Filtering by date, source, or relevance\n",
    "\n",
    "\n",
    "In our example the documents contain one sentence, but in real world application they often include many paragraphs. RAG systems employ the following techniques to help in correct retrieval of information.\n",
    "  - Break large documents into smaller \"chunks\" (paragraphs or sections)\n",
    "  - Overlap chunks to maintain context\n",
    "  - Store metadata (source, date, author) with each chunk\n",
    "\n",
    "In our example, our private database is static, but in the real world RAG systems typically include the following to strengthen responses generated. \n",
    "  - Continuously updated databases\n",
    "  - Real-time web searches\n",
    "  - Fresh information retrieval\n",
    "\n",
    "In our example, we asked one question, retrieved on document and provided one answer. Real application employ the following measures to solve complex questions and utilize multiple sources.\n",
    "  - Break complex questions into sub-questions\n",
    "  - Multiple retrieval rounds\n",
    "  - Synthesize information from multiple sources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
