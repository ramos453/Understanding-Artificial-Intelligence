{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78c0d0f8",
   "metadata": {},
   "source": [
    "__Embeddings for an LLM__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77bbf36",
   "metadata": {},
   "source": [
    "This notebook is a simple introduction to the embeddings used in a large language models (LLMs). Large Language models understand text through self attention. Self attention allows an LLM to understand a word based on the surrounding context.\n",
    "\n",
    "In order to understand context, words in a sentence have to undergo numerical transformations that result in embeddings. There are two key types of embeddings displayed in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905babd",
   "metadata": {},
   "source": [
    "__One Hot Encoding__\n",
    "\n",
    "In One Hot Encoding each word is converted into a unique vector. \n",
    "\n",
    "For example, the words cat, dog and mouse would be represented as: \n",
    "\n",
    "* cat = [1,0,0]\n",
    "* dog = [0,1,0]\n",
    "* mouse = [0,0,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e2c9fc",
   "metadata": {},
   "source": [
    "__Dense Embeddings__\n",
    "\n",
    "Dense embeddings quantify the relationships between words. A dense embedding vector contains the following values. \n",
    "\n",
    "* Q = Query  --------   Query represents the current word's question. It essentially asks \"What I'm I looking for\" \n",
    "* K = Key   ------- Key represents the label of each word. \"This is the information I hold\"  \n",
    "* V = Value  -------   Value represents the actual meaning of the word. \n",
    "\n",
    "This structure allows dense embeddings to provide far more information of a word to an LLM than traditional One Hot Encoding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404e0484",
   "metadata": {},
   "source": [
    "The following cells demonstrate how dense embeddings are made through a simple example. First, we will import the packages needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "f7767b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e4344f",
   "metadata": {},
   "source": [
    "The following example sentence will be analyzed in this notebook. __\"Carlos Alcaraz is a very skilled tennis player and philanthropist\"__. \n",
    "\n",
    "First, the sentence is broken down into tokens which represent the individual words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "8a020667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: \n",
      " ['Carlos', 'Alcaraz', 'is', 'a', 'very', 'skilled', 'tennis', 'player', 'and', 'philanthropist']\n",
      "\n",
      " Number of tokens: 10\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Carlos Alcaraz is a very skilled tennis player and philanthropist\"\n",
    "tokens = sentence.split()\n",
    "\n",
    "print(\"Tokens: \\n\",tokens)\n",
    "print(f\"\\n Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a036693c",
   "metadata": {},
   "source": [
    "These tokens are then converted into vectors. In this example, we will create random vectors to represent each word. In LLM's, these embeddings would come from pre-trained models. The embeddings would have many dimensions. In this example, the embeddings will have only one dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3346596",
   "metadata": {},
   "source": [
    "Example: \n",
    "- ChatGPT models are so quick due to these embeddings. The model doesn't compute them in real time as they have already been defined during training. \n",
    "\n",
    "\n",
    "The following cell generates random vectors for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "45a4359c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for the word 'Carlos':\n",
      " [0.34233201 0.70782322 0.58654392 0.45896529 0.19870387 0.2447379\n",
      " 0.78724402 0.2299572 ] \n",
      "\n",
      "Embedding for the word 'Alcaraz':\n",
      " [0.80863703 0.35922496 0.75924621 0.53979024 0.55067224 0.28280501\n",
      " 0.00340947 0.69813302] \n",
      "\n",
      "Embedding for the word 'is':\n",
      " [0.50789885 0.00332485 0.60721943 0.37638575 0.00801276 0.29489882\n",
      " 0.9204668  0.53738749] \n",
      "\n",
      "Embedding for the word 'a':\n",
      " [0.37905786 0.96145295 0.81727328 0.07657175 0.20016525 0.83958062\n",
      " 0.42651903 0.4139435 ] \n",
      "\n",
      "Embedding for the word 'very':\n",
      " [0.28268319 0.0743492  0.34933177 0.44806844 0.44347801 0.28984714\n",
      " 0.4892609  0.97517145] \n",
      "\n",
      "Embedding for the word 'skilled':\n",
      " [0.11465447 0.69000339 0.2034245  0.23387701 0.43350531 0.52350151\n",
      " 0.54595507 0.48093439] \n",
      "\n",
      "Embedding for the word 'tennis':\n",
      " [0.88419703 0.92647658 0.32072877 0.11380039 0.90957333 0.07126183\n",
      " 0.27856685 0.20602544] \n",
      "\n",
      "Embedding for the word 'player':\n",
      " [0.96184696 0.5915835  0.36829327 0.22900793 0.35531794 0.79177672\n",
      " 0.34428773 0.41831989] \n",
      "\n",
      "Embedding for the word 'and':\n",
      " [0.09486027 0.31221115 0.58181617 0.70155052 0.32164166 0.23523023\n",
      " 0.05827101 0.87532928] \n",
      "\n",
      "Embedding for the word 'philanthropist':\n",
      " [0.08715031 0.1990494  0.99006667 0.87916974 0.22055099 0.97378202\n",
      " 0.52027397 0.34850588] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_model = 8\n",
    "n_tokens = len(tokens)\n",
    "\n",
    "embeddings = np.random.rand(n_tokens, d_model)\n",
    "# include the word next to each embedding\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"Embedding for the word '{token}':\\n\", embeddings[i], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200ac13c",
   "metadata": {},
   "source": [
    "The next step is to create the Query, Key and Value vectors which will allow us to transform the simple embeddings into dense embeddings. At the start of training, these vectors are randomly assigned and gradually adjust to optimal values through training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "803cdc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dimension for our Q, K, and V vectors. It can be different from d_model.\n",
    "d_k = 6\n",
    "# These matrices transform the input embeddings into the Q, K, and V spaces.\n",
    "W_Q = np.random.rand(d_model, d_k)\n",
    "W_K = np.random.rand(d_model, d_k)\n",
    "W_V = np.random.rand(d_model, d_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa66bf5",
   "metadata": {},
   "source": [
    "In order to derive the Query, Key and Value vectors for each word, the word's embedding will be multiplied by each of the Q, K and V vectors. \n",
    "\n",
    "\n",
    "What is this word looking for: \n",
    "- Query Vector = word embedding x Query Vector\n",
    "- Key Vector = word embedding x Key Vector\n",
    "- Value Vector = word embedding x Value Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "2cfc7ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Query matrix: (10, 6)\n",
      "Query \n",
      " [[1.26535811 1.85957245 1.44507856 1.31112859 1.36775768 1.60020967]\n",
      " [1.39511514 2.10762303 2.27040751 1.29268469 1.37557501 1.67435142]\n",
      " [1.15327443 1.86268832 1.67460328 0.76079086 1.52265387 1.48942019]\n",
      " [1.50726343 1.83887757 1.839101   1.85747175 1.14552448 1.65543575]\n",
      " [1.1911837  1.97270956 2.16924636 1.05457209 1.24983044 1.36650191]\n",
      " [1.17377374 1.62665185 1.70560996 1.46941051 0.85771377 1.23256189]\n",
      " [1.27789966 1.73206948 1.65986651 1.29687169 1.16203942 1.56042428]\n",
      " [1.06003181 1.41323641 1.41304266 1.10428475 0.95904985 1.28960101]\n",
      " [1.22735991 1.81388128 1.9411265  1.12634582 1.22361666 1.50542652]\n",
      " [1.738504   2.41037611 2.35744163 1.24566756 1.73405956 2.1560807 ]]\n"
     ]
    }
   ],
   "source": [
    "# Project the embeddings into Q, K, and V spaces\n",
    "Query = embeddings @ W_Q\n",
    "Key = embeddings @ W_K\n",
    "Value = embeddings @ W_V\n",
    "\n",
    "print(\"Shape of Query matrix:\", Query.shape)\n",
    "print(\"Query \\n\", Query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b32228",
   "metadata": {},
   "source": [
    "__Self Attention Scores__ \n",
    "\n",
    "In order to figure out how much attention a word should pay to every other word, we take the dot product of its Query Vector with the Key Vectors of all other words in the sentence. A high score relates to higher relevance. \n",
    "\n",
    "\n",
    "Query: What is this word looking for ? \n",
    "\n",
    "Key: What meaning does this word contain ? \n",
    "\n",
    "- Self Attention Scores = Query Vector of Word x Key Vector of all other words\n",
    "- $Attention Scores = Q * K^{T} /  \\sqrt{D_K}$\n",
    "- Note, this product is divide by the dimensionality or the k vector to prevent the numbers from growing too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "06e7face",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw scores for the word 'tennis':\n",
      "\n",
      " [4.86740936 5.77822634 4.51954615 6.09489065 4.96068668 4.80272987\n",
      " 5.01459764 4.17289351 4.69234047 6.33613172]\n"
     ]
    }
   ],
   "source": [
    "# Calculate raw scores by multiplying Query vectors with Key vectors\n",
    "scores = (Query @ Key.T) / np.sqrt(d_k)\n",
    "\n",
    "print(\"\\nRaw scores for the word 'tennis':\\n\\n\", scores[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d1915",
   "metadata": {},
   "source": [
    "The next step is to apply the SoftMax function for the attention weights since the raw scores are difficult to interpret.\n",
    "\n",
    "The SoftMax function scales numbers into weights that add up to 1. Therefore, a weight of .6 means that a word dedicates 60% of its attention to another word. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "e5a1e538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights for 'tennis' (row 6):\n",
      "[4.87 5.78 4.52 6.09 4.96 4.8  5.01 4.17 4.69 6.34]\n",
      "\n",
      "Let's check the new weights for 'tennis' (row 6):\n",
      "[0.06 0.15 0.04 0.21 0.07 0.06 0.07 0.03 0.05 0.26]\n",
      "\n",
      "Attention weights for 'tennis' (row 6) with corresponding tokens:\n",
      "Carlos: 0.06\n",
      "Alcaraz: 0.15\n",
      "is: 0.04\n",
      "a: 0.21\n",
      "very: 0.07\n",
      "skilled: 0.06\n",
      "tennis: 0.07\n",
      "player: 0.03\n",
      "and: 0.05\n",
      "philanthropist: 0.26\n"
     ]
    }
   ],
   "source": [
    "print(\"Original weights for 'tennis' (row 6):\")\n",
    "print(np.round(scores[6], 2))\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "attention_weights = softmax(scores)\n",
    "\n",
    "print(\"\\nLet's check the new weights for 'tennis' (row 6):\")\n",
    "print(np.round(attention_weights[6], 2))\n",
    "\n",
    "# print the new weights for tennis showing the other tokens \n",
    "print(\"\\nAttention weights for 'tennis' (row 6) with corresponding tokens:\")\n",
    "for token, weight in zip(tokens, np.round(attention_weights[6], 2)):\n",
    "    print(f\"{token}: {weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceea9f4",
   "metadata": {},
   "source": [
    "Based on the results above, the word tennis would pay most attention to the words 'Philanthropist' and 'a'. Note, this may change when running notebook as the vectors are randomly assigned each time the notebook is ran.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e92d7ef",
   "metadata": {},
   "source": [
    "Lastly, we take the weighted sum of all Value vectors, using the attention weights. Therefore, words with higher attention weights contribute more to the final value vector for the current word. This final output is a __context aware__ representation of each word. \n",
    "\n",
    "- Value vector represents the actual meaning of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "f829cb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original embedding for 'tennis':\n",
      " [0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1]\n",
      "\n",
      "New context-aware output vector for 'tennis':\n",
      " [1.46 1.7  1.85 1.69 1.37 1.91]\n"
     ]
    }
   ],
   "source": [
    "# The final output is a weighted sum of the Value vectors\n",
    "output = attention_weights @ Value\n",
    "\n",
    "print(\"\\nOriginal embedding for 'tennis':\\n\", np.round(embeddings[6], 2))\n",
    "print(\"\\nNew context-aware output vector for 'tennis':\\n\", np.round(output[6], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f3f17",
   "metadata": {},
   "source": [
    "__Conclusion & Real Life Applications__ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbbf481",
   "metadata": {},
   "source": [
    "This simple mechanism of representing words through context aware vectors is the building block for LLM's. Advance LLM's use multi-head attention which doesn't just ask one question (Query), but asks many question such as \"What is the grammar related to this word?\" and \"What is the topic of this word?\". This is like having multiple people analyze the same sentence from different perspectives.\n",
    "\n",
    "LLM's like Chat-GPT or Gemini stack multiple layers of what we just did. Therefore, the context rich output from layer 1 becomes the input for the second layer and so on. By the time, the text has passed through all of the layers, the model has built and incredibly deep understanding of the relationships between every word. This allows an LLM to generate coherent and relevant responses. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
