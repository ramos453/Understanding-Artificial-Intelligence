{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7f9a9d7",
   "metadata": {},
   "source": [
    "__RAG Quality Benchmarking with RAGAs__\n",
    "\n",
    "The goal of this project is to evaluate a RAG pipeline using an automated, LLM-as-a-judge. Therefore, determining the faithfulness and answer relevancy of the generated answers. \n",
    "\n",
    "__RAGAs (Retrieval- Augmented Generation Assessment System)__\n",
    "- Framework designed to evaluate the responses of a RAG system\n",
    "- Acts as a judge to determine the quality of the answers\n",
    "\n",
    "__Key Areas of Performance__\n",
    "1. Faithfulness (Is the answer true to the source/private database)\n",
    "2. Answer Relevancy (Does the answer fit the question?) \n",
    "3. Context Retrieval Quality (Did it find the right information?)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4455c7",
   "metadata": {},
   "source": [
    "The next cell imports the necessary libraries needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "018b2b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy\n",
    "import os\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccffe56a",
   "metadata": {},
   "source": [
    "The next cell sets up the evaluator, which will judge the responses generated by a RAG system. In this example we are using the OpenAI model **GPT-3.5 Turbo** with a temperature of zero. \n",
    "\n",
    "**Temperature** controls the models creativity. A value of 0 will introduce less randomness to the model's output. This is desired since the evaluator should be provide consistent scoring. \n",
    "\n",
    "Note, the connection to this model private and therefore running this notebook will not work unless you set up your own connection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66ed3a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models RAGAs will use as the 'Judge'\n",
    "# RAGAs uses an LLM to judge the quality of other LLM outputs.\n",
    "evaluator_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "evaluator_embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02642b99",
   "metadata": {},
   "source": [
    "This notebook is focused on evaluating RAG outputs only. Therefore, we will not implement a RAG system from scratch but evaluate mock data representing RAG system.\n",
    "\n",
    "\n",
    "The mock data below represents sample **questions, answers, contexts, and ground truths** gathered from a RAG system. \n",
    "\n",
    "The mock data includes three sets of records, two of which are good and one is bad. The goal of this project is to determine if our RAG evaluator, an LLM, is able to spot the bad record. Below is an example of a \"Good\" record which represent a well generated response to a given question as well as a bad example. \n",
    "\n",
    "\n",
    "\n",
    "**Good**\n",
    "\n",
    "1. **Question:** What is the Capital of France.\n",
    "\n",
    "\n",
    "2. **Answer:** The Capital of France is Paris, famous for the Eiffel Tower. \n",
    "\n",
    "\n",
    "3. **Contexts:** The main city of France, Paris, has many landmarks. Paris is a major European center. \n",
    "\n",
    "\n",
    "4. **Ground Truth:** Paris is the Capital of France.\n",
    "\n",
    "\n",
    "**Bad** \n",
    "\n",
    "1. **Question:** When was the World Wide Web invented.\n",
    "\n",
    "\n",
    "2. **Answer:** <span style=\"color: red;\">The context does not specify the exact invention date of the World Wide Web.</span>   \n",
    "\n",
    "\n",
    "3. **Contexts:** Tim Berners-Lee created a distributed information system at CERN in 1989.\n",
    "\n",
    "\n",
    "4. **Ground Truth:** The World Wide Web was invented in 1989 by Tim Berners-Lee at CERN.\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "The \"Bad\" example represents a failure by the RAG to provide a correct answer, even though it had the necessary information to answer correctly. The __Contexts__ provided to the RAG system include \"Tim Berners-Lee created a distributed information system at CERN in 1989.\", therefore the RAG system should have responsed that the World Wide Web was created in 1989. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52833d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data simulates the output of your RAG system after running a few test questions.\n",
    "# The 'ground_truths' are the human-written, correct answers (required for some metrics).\n",
    "mock_data = {\n",
    "    'question': [\n",
    "        \"What is the capital of France?\",\n",
    "        \"How much does a light-year measure in kilometers?\",\n",
    "        \"When was the World Wide Web invented?\",\n",
    "    ],\n",
    "    'answer': [\n",
    "        \"The capital of France is Paris, famous for the Eiffel Tower.\",\n",
    "        \"The light-year measures about 9.461 trillion kilometers.\",\n",
    "        \"The context does not specify the exact invention date of the World Wide Web.\",\n",
    "    ],\n",
    "    # 'contexts' are the specific chunks the RAG system retrieved.\n",
    "    'contexts': [\n",
    "        [\"The main city of France, Paris, has many landmarks. Paris is a major European center.\"],\n",
    "        [\"A light-year is the distance light travels in one year, which is 9.461e12 kilometers.\"],\n",
    "        [\"Tim Berners-Lee created a distributed information system at CERN in 1989.\"], # Intentionally missing an explicit date\n",
    "    ],\n",
    "    # 'ground_truths' are the perfect, human-verified answers (important for recall-based metrics)\n",
    "    'ground_truths': [\n",
    "        [\"Paris is the capital of France.\"],\n",
    "        [\"A light-year is approximately $9.461 \\times 10^{12}$ kilometers (9.461 trillion).\"],\n",
    "        [\"The World Wide Web was invented in 1989 by Tim Berners-Lee at CERN.\"],\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert the dictionary into a RAGAs-compatible Dataset\n",
    "eval_dataset = Dataset.from_dict(mock_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a50cf5",
   "metadata": {},
   "source": [
    "The following cell provides the evaluator with the mock data and specifies faithfulness and answer relevancy as the metrics for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bda006d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RAGAs evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 6/6 [00:09<00:00,  1.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# --- Define the metrics to use ---\n",
    "metrics_to_evaluate = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "]\n",
    "\n",
    "# --- Run the Evaluation ---\n",
    "print(\"Starting RAGAs evaluation...\")\n",
    "results = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=metrics_to_evaluate,\n",
    "    llm=evaluator_llm,\n",
    "    embeddings=evaluator_embeddings\n",
    ")\n",
    "\n",
    "# Convert results to a pandas DataFrame for clear viewing\n",
    "results_df = results.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e148929a",
   "metadata": {},
   "source": [
    "The following cell prints the results of the evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcfe2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>[The main city of France, Paris, has many landmarks. Paris is a major European center.]</td>\n",
       "      <td>The capital of France is Paris, famous for the Eiffel Tower.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How much does a light-year measure in kilometers?</td>\n",
       "      <td>[A light-year is the distance light travels in one year, which is 9.461e12 kilometers.]</td>\n",
       "      <td>The light-year measures about 9.461 trillion kilometers.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.939068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When was the World Wide Web invented?</td>\n",
       "      <td>[Tim Berners-Lee created a distributed information system at CERN in 1989.]</td>\n",
       "      <td>The context does not specify the exact invention date of the World Wide Web.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0                     What is the capital of France?   \n",
       "1  How much does a light-year measure in kilometers?   \n",
       "2              When was the World Wide Web invented?   \n",
       "\n",
       "                                                                        retrieved_contexts  \\\n",
       "0  [The main city of France, Paris, has many landmarks. Paris is a major European center.]   \n",
       "1  [A light-year is the distance light travels in one year, which is 9.461e12 kilometers.]   \n",
       "2              [Tim Berners-Lee created a distributed information system at CERN in 1989.]   \n",
       "\n",
       "                                                                       response  \\\n",
       "0                  The capital of France is Paris, famous for the Eiffel Tower.   \n",
       "1                      The light-year measures about 9.461 trillion kilometers.   \n",
       "2  The context does not specify the exact invention date of the World Wide Web.   \n",
       "\n",
       "   faithfulness  answer_relevancy  \n",
       "0           0.0          1.000000  \n",
       "1           1.0          0.939068  \n",
       "2           1.0          0.000000  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c2b07",
   "metadata": {},
   "source": [
    "__Faithfulness__ \n",
    "- Measures if the generated answer is supported by the retrieved contexts. \n",
    "- 1 = perfect score. The answer is supported in the retrieved contexts.\n",
    "- 0 = worst score. The answer is a hallucination.\n",
    "\n",
    "\n",
    "__Answer Relevancy__ \n",
    "- Measures if the answer directly addresses the question. \n",
    "- 1 = perfect score. The answer addresses the question fully. \n",
    "- 0 = worst score. The answer is incomplete or does not answer the question at hand. \n",
    "\n",
    "\n",
    "Based on this understanding of Faithfulness and Answer Relevancy. \n",
    "\n",
    "__Question 1: What is the Capital of France ?__\n",
    "- **Retrieved Contexts:** \"The __main__ city of France, Paris\"\n",
    "- **Response:** \"The __capital__ of France is Paris\" \n",
    "- **Faithfulness:** The answer is not supported by the retrieve contexts due to the difference in __main__ and __capital__. The retrieved contexts don't specify that Paris is the capital of France. It only mentions that Paris is a main City. \n",
    "- **Answer Relevancy:** The answer completely addresses the questions by stating that Paris is the capital of France. \n",
    "\n",
    "\n",
    "__Question 2: How much does a light-year measure in Kilometers ?__\n",
    "- **Retrieved Contexts:** \"A light-year is the distance light travels in one year, which is 9.461e12 kilometers\"\n",
    "- **Response:** \"The light-year measures about 9.461 trillion kilometers.\" \n",
    "- **Faithfulness:** The answer is supported by the retrieved contexts. It specifically mentions the 9.461 trillion kilometers seen in the retrieved contexts.\n",
    "- **Answer Relevancy:** The answer addresses the questions by stating the exact distance. The score is not 1, but relatively high at .939.  \n",
    "\n",
    "\n",
    "__Question 3: When was the World Wide Web invented ?__\n",
    "- **Retrieved** Contexts: \"Tim Berners-Lee created a distributed information system at CERN in 1989\"\n",
    "- **Response:** __\"The context does not specify the exact invention date of the World Wide Web\"__ \n",
    "- **Faithfulness:** The answer is supported by the retrieved contexts because the question asks specifically for the __World Wide Web__, however the retrieved contexts only mentions the __distributed information system__. \n",
    "- **Answer Relevancy:** The answer fails to provide an answer for the question.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e437d07",
   "metadata": {},
   "source": [
    "__Conclusion__\n",
    "- The evaluator succeeds in pointing out specific issues in the answers. However, it fails to understand synonyms within the text. \n",
    "\n",
    "- __First Example:__ The difference in the words \"main\" and \"capital\" are not the same in the eyes of the evaluator. However, this is useful information and can help us better the context provided we provide to the RAG system. \n",
    "\n",
    "- __Third Example:__ The difference in the phrases \"World Wide Web\" and \"Distributed Information System\" are not the same in the eyes of the evaluator. However, this is useful information and can help us better the context provided we provide to the RAG system. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcdc8cb",
   "metadata": {},
   "source": [
    "__Additional Considerations__\n",
    "\n",
    "In the real world, RAGAs are fully trained and automated assurance testers for AI Chatbots. This is crucial for startups and companies due liability and costs of bad answers. A company does not have time to proof read a million questions manually to ensure the RAG system is not hallucinating, therefore they must have an automated solution. Using an LLM as a judge reduces the risk of hallucinations and cost of manually proofreading responses. RAGAs evaluate based on faithfulness, answer relevancy and context precision. This notebook only covered faithfulness and answer relevancy. However, context precision measures if the LLM was able to find and use only the most useful documents in order to answer the question at hand. If context precision is poor, companies spend more resources and time in order to generate a correct response. RAGAs are also used in safeguarding users from new models. A new model will undergo RAGAs tests and if faithfulness, answer relevancy or context precision drop, users will not be exposed to this model as it is unsafe to use.  \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_quality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
