{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bbb191a",
   "metadata": {},
   "source": [
    "__Tokens, Context Windows and Chunking__\n",
    "\n",
    "The goal of this notebook is to explain the basics of tokens, context windows, and chunking.\n",
    "\n",
    "__Tokens__\n",
    "\n",
    "Tokens are small pieces of text that an AI model, such as an LLM, processes. Tokens are represented by a series of integers that correspond to entries in a predefined vocabulary. Many modern LLMs use subword tokenization such as Byte-Pair Encoding or WordPiece. By creating tokens from subwords instead of entire words, the vocabulary does not need to grow without bound. This also allows models to handle text outside the predefined vocabulary. Tokenization strongly affects performance and cost. In this example we use tiktoken, which is a tokenizer compatible with the gpt-4o model.\n",
    "\n",
    "__Context Window__\n",
    "\n",
    "Context windows define the maximum number of tokens a model can process in a single forward pass. The self-attention mechanism used in LLMs scales roughly quadratically in computational and memory cost as context grows, which makes very large contexts expensive. Increasing a model's context window involves trade-offs: higher latency and greater compute and memory usage. A common issue is the \"lost in the middle\" problem, where models that ingest long documents struggle to recall or use information from the middle of the context. In this notebook we use gpt-3.5-turbo-0125 (context window: 16,385 tokens) to demonstrate exceeding the context limit and how chunking can help.\n",
    "\n",
    "__Chunking__\n",
    "\n",
    "Chunking is the process of splitting text into smaller, more manageable pieces for a RAG (Retrieval-Augmented Generation) system. Chunking affects a RAG system's ability to efficiently and accurately synthesize context and impacts downstream performance. In this example we implement fixed-size chunking, the simplest approach. Other approaches include Recursive Character Splitting and Semantic Chunking. Recursive Character Splitting looks for natural boundaries such as newlines and paragraph breaks. Semantic chunking leverages embeddings to split text based on semantic similarity.\n",
    "\n",
    "\n",
    "<span style=\"color:darkblue\">El objetivo de este cuaderno es explicar los conceptos b√°sicos de tokens, ventanas de contexto y chunking.</span>\n",
    "\n",
    "<span style=\"color:darkblue\">__Tokens:__ Los tokens son peque√±os fragmentos de texto que un modelo de IA, como un LLM, procesa. Los tokens se representan mediante una serie de enteros que corresponden a entradas en un vocabulario predefinido. Muchos LLM modernos usan tokenizaci√≥n por subpalabras, por ejemplo Byte-Pair Encoding o WordPiece. Al crear tokens a partir de subpalabras en lugar de palabras completas, el vocabulario no necesita crecer indefinidamente. Esto tambi√©n permite que los modelos manejen texto fuera del vocabulario predefinido. La tokenizaci√≥n afecta en gran medida el rendimiento y el costo. En este ejemplo usamos tiktoken, un tokenizador compatible con el modelo gpt-4o.</span>\n",
    "\n",
    "<span style=\"color:darkblue\">__Ventana de contexto:__ Las ventanas de contexto definen el n√∫mero m√°ximo de tokens que un modelo puede procesar en una sola pasada hacia adelante. El mecanismo de self-attention que usan los LLM escala aproximadamente de forma cuadr√°tica en coste computacional y de memoria al aumentar el contexto, lo que hace que los contextos muy grandes sean costosos. Aumentar la ventana de contexto de un modelo implica compensaciones: mayor latencia y mayor uso de c√≥mputo y memoria. Un problema habitual es el \"perderse en el medio\", donde los modelos que ingieren documentos largos tienen dificultades para recordar o usar la informaci√≥n que aparece en la parte central del contexto. En este cuaderno usamos gpt-3.5-turbo-0125 (ventana de contexto: 16,385 tokens) para demostrar exceder el l√≠mite de contexto y c√≥mo el chunking puede ayudar.</span>\n",
    "\n",
    "<span style=\"color:darkblue\">__Chunking:__ El chunking es el proceso de dividir el texto en piezas m√°s peque√±as y manejables para un sistema RAG (Generaci√≥n Aumentada por Recuperaci√≥n). El chunking afecta la capacidad del sistema RAG para sintetizar el contexto de forma eficiente y precisa e impacta el rendimiento posterior. En este ejemplo implementamos chunking de tama√±o fijo, que es el enfoque m√°s sencillo. Otros enfoques incluyen Recursive Character Splitting y Semantic Chunking. Recursive Character Splitting busca l√≠mites naturales como saltos de l√≠nea y p√°rrafos. El chunking sem√°ntico utiliza embeddings para dividir el texto seg√∫n la similitud sem√°ntica.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb38afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import openai\n",
    "import tiktoken\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8120a3d6",
   "metadata": {},
   "source": [
    "For this example we will use the gpt-4o model from OpenAI. The following cell creates two text inputs and prints their respective tokens.\n",
    "\n",
    "<span style=\"color:darkblue\">Para este ejemplo usaremos el modelo gpt-4o de OpenAI. La celda siguiente crea dos entradas de texto e imprime sus tokens correspondientes.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a67dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'Hello, moto !'\n",
      "Tokens: [13225, 11, 37906, 1073]\n",
      "Number of tokens: 4\n",
      "\n",
      "Text: 'Mexico will win the World Cup in 2026.'\n",
      "Tokens: [134721, 738, 4449, 290, 5922, 17257, 306, 220, 1323, 21, 13]\n",
      "Number of tokens: 11\n"
     ]
    }
   ],
   "source": [
    "# tokenizer for the gpt-4o model\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "# A simple sentence\n",
    "text1 = \"Hello, moto !\"\n",
    "tokens1 = encoding.encode(text1)\n",
    "print(f\"Text: '{text1}'\")\n",
    "print(f\"Tokens: {tokens1}\")\n",
    "print(f\"Number of tokens: {len(tokens1)}\\n\")\n",
    "\n",
    "# A more complex sentence\n",
    "text2 = \"Mexico will win the World Cup in 2026.\"\n",
    "tokens2 = encoding.encode(text2)\n",
    "print(f\"Text: '{text2}'\")\n",
    "print(f\"Tokens: {tokens2}\")\n",
    "print(f\"Number of tokens: {len(tokens2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e384ffa7",
   "metadata": {},
   "source": [
    "In the next cell we will generate a long fake document by repeating a sample sentence 20,000 times. Then we will pass this content to the gpt-3.5-turbo-0125 model from OpenAI, which has a maximum context length of 16,385 tokens. The goal is to have the model summarize the long fake document (‚âà50,000 tokens); we expect an error because the context length will be exceeded.\n",
    "\n",
    "<span style=\"color:darkblue\">En la siguiente celda generaremos un documento falso largo repitiendo una frase de ejemplo 20,000 veces. Luego pasaremos este contenido al modelo gpt-3.5-turbo-0125 de OpenAI, que tiene una longitud m√°xima de contexto de 16,385 tokens. El objetivo es que el modelo resuma el documento falso largo (‚âà50,000 tokens); esperamos un error porque se exceder√° la longitud de contexto.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a519eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our sample text has approximately 160001 tokens.\n",
      "\n",
      "üí• We got an error, as expected!\n",
      "Error Message: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 160014 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "# Let's create a very long string of text\n",
    "# Let's simulate a ~40-page document, which will be well over the limit.\n",
    "long_text = \"Harry Potter, you are a wizard. \" * 20000 # ~200k characters, ~50k tokens\n",
    "num_tokens = len(encoding.encode(long_text))\n",
    "\n",
    "print(f\"Our sample text has approximately {num_tokens} tokens.\")\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo-0125\",\n",
    "      messages=[\n",
    "        {\"role\": \"user\", \"content\": f\"Summarize this text: {long_text}\"}\n",
    "      ]\n",
    "    )\n",
    "except openai.BadRequestError as e:\n",
    "    print(\"\\nüí• We got an error, as expected!\")\n",
    "    print(f\"Error Message: {e.message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c14d802",
   "metadata": {},
   "source": [
    "In the next cell we will implement fixed-size chunking to split the content so it can be provided to the model. Chunking allows long documents to be processed by LLMs in smaller pieces.\n",
    "\n",
    "<span style=\"color:darkblue\">En la siguiente celda implementaremos chunking de tama√±o fijo para dividir el contenido y poder proporcionarlo al modelo. El chunking permite que los LLM procesen documentos largos en piezas m√°s peque√±as.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "507adf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The long text was split into 161 chunks.\n",
      "The first chunk has 1000 tokens.\n",
      "\n",
      "Here's the first chunk:\n",
      "---\n",
      "Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard. Harry Potter, you are a wizard.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size_tokens):\n",
    "    \"\"\"Splits a text into chunks of a specified token size.\"\"\"\n",
    "    tokens = encoding.encode(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size_tokens):\n",
    "        chunk = tokens[i:i + chunk_size_tokens]\n",
    "        chunks.append(encoding.decode(chunk))\n",
    "    return chunks\n",
    "\n",
    "# Let's set a chunk size that is safely within the context window\n",
    "CHUNK_SIZE = 1000\n",
    "\n",
    "# Chunk our long text\n",
    "text_chunks = chunk_text(long_text, CHUNK_SIZE)\n",
    "\n",
    "print(f\"The long text was split into {len(text_chunks)} chunks.\")\n",
    "print(f\"The first chunk has {len(encoding.encode(text_chunks[0]))} tokens.\")\n",
    "print(\"\\nHere's the first chunk:\\n---\")\n",
    "print(text_chunks[0])\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6181b67",
   "metadata": {},
   "source": [
    "The following cell shows a breakdown of costs for the gpt-3.5-turbo-0125 model based on the estimated tokens per chunk and the number of chunks. This cost analysis is important when designing RAG systems: the value the RAG system provides should exceed the operational cost.\n",
    "\n",
    "<span style=\"color:darkblue\">La celda siguiente muestra un desglose de costes para el modelo gpt-3.5-turbo-0125 basado en los tokens estimados por chunk y el n√∫mero de chunks. Este an√°lisis de costes es importante al dise√±ar sistemas RAG: el valor que proporciona el sistema RAG debe superar el coste operativo.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e701c544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Input Tokens: 160001\n",
      "Estimated Total Output Tokens: 8050\n",
      "---\n",
      "Estimated Input Cost: $0.0800\n",
      "Estimated Output Cost: $0.0121\n",
      "ESTIMATED TOTAL COST: $0.0921\n"
     ]
    }
   ],
   "source": [
    "# Pricing for gpt-3.5-turbo-0125 as an example\n",
    "INPUT_PRICE_PER_1M_TOKENS = 0.50  # $0.50\n",
    "OUTPUT_PRICE_PER_1M_TOKENS = 1.50 # $1.50\n",
    "\n",
    "# 1. Calculate total input tokens\n",
    "total_input_tokens = sum(len(encoding.encode(chunk)) for chunk in text_chunks)\n",
    "\n",
    "# 2. Estimate total output tokens (let's assume each summary is ~50 tokens)\n",
    "estimated_output_tokens_per_chunk = 50\n",
    "total_output_tokens = len(text_chunks) * estimated_output_tokens_per_chunk\n",
    "\n",
    "# 3. Calculate cost\n",
    "input_cost = (total_input_tokens / 1_000_000) * INPUT_PRICE_PER_1M_TOKENS\n",
    "output_cost = (total_output_tokens / 1_000_000) * OUTPUT_PRICE_PER_1M_TOKENS\n",
    "total_cost = input_cost + output_cost\n",
    "\n",
    "print(f\"Total Input Tokens: {total_input_tokens}\")\n",
    "print(f\"Estimated Total Output Tokens: {total_output_tokens}\")\n",
    "print(\"---\")\n",
    "print(f\"Estimated Input Cost: ${input_cost:.4f}\")\n",
    "print(f\"Estimated Output Cost: ${output_cost:.4f}\")\n",
    "print(f\"ESTIMATED TOTAL COST: ${total_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf2aa6",
   "metadata": {},
   "source": [
    "The total estimated cost for this simple example is about $0.09. While this seems inexpensive for a single run, costs can increase significantly depending on how often documents are referenced or reprocessed.\n",
    "\n",
    "<span style=\"color:darkblue\">El coste estimado total para este ejemplo simple es de aproximadamente $0.09. Aunque parece econ√≥mico para una sola ejecuci√≥n, los costes pueden aumentar significativamente seg√∫n la frecuencia con la que se consulten o reprocesen los documentos.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cdffd4",
   "metadata": {},
   "source": [
    "__Additional Considerations__\n",
    "\n",
    "Here's the text with improved line breaks while maintaining the blue spans:\n",
    "\n",
    "__Tokenization__ impacts performance and costs. In practice, language differences and industry-specific vocabulary influence which tokenizer is best for a use case. For example, a single Japanese character may be split into many tokens, inflating token counts and increasing costs while potentially reducing model performance. Similar token inflation can occur with specialized vocabulary in domains such as law or medicine. Some organizations choose to build custom tokenizers tailored to their domain to reduce cost and improve model outputs.\n",
    "\n",
    "__Context Windows__ often encounter the \"lost in the middle\" problem, where a RAG system may be less effective at using information that appears in the middle of a long context. To mitigate this, systems commonly use re-ranking algorithms that select and rank the most relevant chunks using a smaller model; the final query includes the top-ranked chunks first, improving accuracy.\n",
    "\n",
    "__Chunking__ is essential to provide effective context to RAG systems. There are several chunking approaches, such as Recursive Character Splitting, Semantic Chunking, and Layout-Aware Chunking. Layout-Aware Chunking preserves elements of complex documents (tables, headings, paragraphs), producing chunks that better reflect how humans interpret the document. By feeding these structured chunks into a RAG system, retrieval quality and downstream performance can improve significantly.\n",
    "\n",
    "<span style=\"color:darkblue\">__Consideraciones adicionales:__</span>\n",
    "\n",
    "<span style=\"color:darkblue\">__Tokenizaci√≥n:__ La tokenizaci√≥n impacta el rendimiento y los costes. En la pr√°ctica, las diferencias entre idiomas y el vocabulario espec√≠fico de cada sector influyen en qu√© tokenizador es m√°s adecuado para un caso de uso. Por ejemplo, un √∫nico car√°cter japon√©s puede dividirse en muchos tokens, inflando el recuento de tokens y aumentando los costes, adem√°s de poder reducir el rendimiento del modelo. Un inflado similar puede suceder con vocabulario especializado en dominios como el jur√≠dico o el m√©dico. Algunas organizaciones optan por crear tokenizadores personalizados adaptados a su dominio para reducir costes y mejorar los resultados del modelo.</span>\n",
    "\n",
    "<span style=\"color:darkblue\">__Ventanas de contexto:__ A menudo se encuentra el problema de \"perderse en el medio\", donde un sistema RAG puede ser menos efectivo al utilizar informaci√≥n que aparece en la parte central de un contexto largo. Para mitigarlo, los sistemas suelen usar algoritmos de reordenamiento (re-ranking) que seleccionan y ordenan los chunks m√°s relevantes mediante un modelo m√°s peque√±o; la consulta final incluye primero los chunks mejor clasificados, lo que mejora la precisi√≥n.</span>\n",
    "\n",
    "<span style=\"color:darkblue\">__Chunking:__ El chunking es esencial para proporcionar contexto efectivo a los sistemas RAG. Existen varios enfoques, como Recursive Character Splitting, Semantic Chunking y Layout-Aware Chunking. El Layout-Aware Chunking preserva elementos de documentos complejos (tablas, t√≠tulos, p√°rrafos), generando chunks que reflejan mejor c√≥mo los humanos interpretan el documento. Al alimentar estos chunks estructurados en un sistema RAG, la calidad de la recuperaci√≥n y el rendimiento posterior pueden mejorar significativamente.</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_quality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
